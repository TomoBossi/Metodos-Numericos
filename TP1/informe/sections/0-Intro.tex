\section{Introducción}

Cualquier sistema de ecuaciones lineales puede ser representado en forma matricial como $Ax=b$, donde $A$ es la matriz de los coeficientes que acompañan a cada una de las incognitas, $x$ es el vector de incognitas y $b$ es el vector de términos independientes. Resolver un sistema de ecuaciones lineales significa, entonces, dados $A$ y $b$ encontrar $x$ tal que $Ax=b$. Son particularmente interesantes los sistemas con igual cantidad de ecuaciones que incognitas, representables por una matriz de coeficientes $A$ cuadrada, que pueden según $A$ y $b$ no tener solución, tener solución única, o tener infinitas soluciones.

Existen algoritmos que permiten hallar la solución única de un dado sistema de ecuaciones, si es que tal solución existe. Los más conocidos y sencillos de estos algoritmos son los de eliminación gaussiana (EG), que en sus versiones más básicas consisten en realizar operaciones elementales entre filas que convierten al sistema en sistemas equivalentes (con el mismo vector solución $x$) progresivamente simplificados. Una vez que se llega a un sistema equivalente suficientemente sencillo, que en general consiste en un sistema en el que la matriz de coeficientes es triangular superior, los elementos de $x$ son despejados uno por uno. Todos los algoritmos de EG se basan en la operación elemental de reemplazo de una fila por la resta entre sí misma y un múltiplo de otra. Algunos algoritmos usan además la operación elemental de intercambio entre filas para resolver sistemas de otra manera irresolubles y disminuir el error numérico en las soluciones halladas, a lo que se le denomina ''pivoteo parcial''\cite{burden}\cite{metnum_EG}.

Los algoritmos de EG de propósito general, diseñados para sistemas dados por cualquier matriz de coeficientes cuadrada, son de complejidad $O(n^3)$. Para matrices ralas (con la mayoría de sus elementos nulos) pertenecientes a una dada familia puede ser posible diseñar algoritmos de una complejidad menor. Un caso trivial es el de los sistemas diagonales de solución única (con matriz diagonal, sin ceros en la diagonal), que ya están prácticamente resueltos. Estos sistemas requieren de a lo sumo $n$ divisiones para hallar $x$, y son por lo tanto de complejidad $O(n)$. Para sistemas tridiagonales (con matriz de coeficientes tridiagonal), se sabe que puede diseñarse un algoritmo que también es de complejidad $O(n)$.

El desarrollo de este trabajo puede ser dividido en cuatro partes. En la primera (sección \ref{sistemas_de_ecuaciones}) se diseñaron algoritmos de EG de propósito general sin y con pivoteo parcial (\ref{sistemas_de_ecuaciones_EG_sin} y \ref{sistemas_de_ecuaciones_EG_con}) y se analizaron casos particulares de uso interesantes (\ref{sistemas_de_ecuaciones_casos}). En la segunda parte (sección \ref{EG_tridiagonales}) se desarrolló un algoritmo de complejidad $O(n)$ específico a sistemas tridiagonales (\ref{EG_tridiagonales_sin_precomputo}), y se lo mejoró separándolo en una primera etapa de precómputo y una segunda etapa de resolución de el o los sistemas lineales dados (\ref{EG_tridiagonales_con_precomputo}). En la tercera parte (sección \ref{tiempos}) se realizaron análisis de los tiempos de cómputo de los algoritmos diseñados, estudiando entre otras cosas su dependencia con el tamaño de las matrices input y verificando que sus complejidades algorítmicas sean las esperadas. Finalmente, la cuarta parte del desarrollo (sección \ref{aplicaciones}) explora aplicaciones de los sistemas tridiagonales, en particular a los problemas de la búsqueda de funciones a partir de sus derivadas segundas (\ref{aplicaciones_laplaciano}) y al de la difusión en el tiempo y el espacio en una dimensión (\ref{aplicaciones_difusion}).